{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-COREF Exploration\n",
    "[Paper](https://arxiv.org/pdf/2209.04280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Ryan Lee\\\\Desktop\\\\AISG Internship\\\\rag'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current file's directory (e.g., the 'notebooks' directory)\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# Navigate one level up to project directory\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# Add the directory to sys.path\n",
    "sys.path.append(project_dir)\n",
    "os.chdir(project_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan Lee\\.conda\\envs\\aiip\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel1 = PatchedLingMessCoref(\\n    nlp=\"en_core_web_lg\",\\n    device=\"cpu\"\\n)\\n\\nmodel2 = PatchedFCoref(\\n    nlp=\"en_core_web_lg\",\\n    device=\"cpu\"\\n)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install coref\n",
    "# Need to patch for our version of transformers (4.44.2)\n",
    "\n",
    "from fastcoref import LingMessCoref as OriginalLingMessCoref\n",
    "from fastcoref import FCoref as OriginalFCoref\n",
    "from transformers import AutoModel\n",
    "import functools\n",
    "\n",
    "class PatchedLingMessCoref(OriginalLingMessCoref):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        original_from_config = AutoModel.from_config\n",
    "\n",
    "        def patched_from_config(config, *args, **kwargs):\n",
    "            kwargs['attn_implementation'] = 'eager'\n",
    "            return original_from_config(config, *args, **kwargs)\n",
    "\n",
    "        try:\n",
    "            AutoModel.from_config = functools.partial(patched_from_config, attn_implementation='eager')\n",
    "            super().__init__(*args, **kwargs)\n",
    "        finally:\n",
    "            AutoModel.from_config = original_from_config\n",
    "\n",
    "class PatchedFCoref(OriginalFCoref):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        original_from_config = AutoModel.from_config\n",
    "\n",
    "        def patched_from_config(config, *args, **kwargs):\n",
    "            kwargs['attn_implementation'] = 'eager'\n",
    "            return original_from_config(config, *args, **kwargs)\n",
    "\n",
    "        try:\n",
    "            AutoModel.from_config = functools.partial(patched_from_config, attn_implementation='eager')\n",
    "            super().__init__(*args, **kwargs)\n",
    "        finally:\n",
    "            AutoModel.from_config = original_from_config\n",
    "                    \n",
    "'''\n",
    "model1 = PatchedLingMessCoref(\n",
    "    nlp=\"en_core_web_lg\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "model2 = PatchedFCoref(\n",
    "    nlp=\"en_core_web_lg\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "'''\n",
    "\n",
    "# Run your stuff here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan Lee\\.conda\\envs\\aiip\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "11/22/2024 17:01:57 - INFO - \t missing_keys: []\n",
      "11/22/2024 17:01:57 - INFO - \t unexpected_keys: []\n",
      "11/22/2024 17:01:57 - INFO - \t mismatched_keys: []\n",
      "11/22/2024 17:01:57 - INFO - \t error_msgs: []\n",
      "11/22/2024 17:01:57 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "11/22/2024 17:01:57 - INFO - \t Tokenize 4 inputs...\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 117.63 examples/s]\n",
      "11/22/2024 17:01:58 - INFO - \t ***** Running Inference on 4 texts *****\n",
      "Inference: 100%|██████████| 4/4 [00:00<00:00,  5.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CorefResult(text=\"We are AISG. We are so happy to see you using the ...\", clusters=[['We', 'We'], ['the coref package', 'This package']]),\n",
       " CorefResult(text=\"Alice goes down the rabbit hole. Where she would d...\", clusters=[['Alice', 'she', 'her']]),\n",
       " CorefResult(text=\"Mary saw Susan at the park. She was playing with a...\", clusters=[['Susan', 'She']]),\n",
       " CorefResult(text=\"Alice went to the library because she wanted to bo...\", clusters=[['Alice', 'she', 'She', 'Alice', 'she', 'her', 'her', 'she', 'Alice'], ['a novel by Kenrick', 'it', 'it'], ['her friend Clara, who asked her what she had borrowed', 'Clara'], ['Kenrick', 'the author']])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install coref\n",
    "\n",
    "model = PatchedFCoref(device='cpu')\n",
    "\n",
    "texts = [\n",
    "    'We are AISG. We are so happy to see you using the coref package. This package is very fast!',\n",
    "    'Alice goes down the rabbit hole. Where she would discover a new reality beyond her expectations.',\n",
    "    'Mary saw Susan at the park. She was playing with a frisbee. They then conversed.',\n",
    "    'Alice went to the library because she wanted to borrow a book. She found a novel by Kenrick and decided to check it out. As Alice walked home, she bumped into her friend Clara, who asked her what she had borrowed. Alice showed it to Clara, and they talked about the author for a while.'\n",
    "]\n",
    "\n",
    "preds = model.predict(\n",
    "   texts=texts\n",
    ")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (13, 15)], [(46, 63), (65, 77)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].get_clusters(as_strings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We', 'We'], ['the coref package', 'This package']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].get_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "- Need to adapt code to work with our version of transformers ([link](https://github.com/shon-otmazgin/fastcoref/issues/59))\n",
    "- `LingMessCoref` is the larger s2e model - bigger input size (Longformer: 4096 tokens) but slower and larger memory footprint. In contrast, `FCoref` (student model via distillation) replaces Longformer with DistilRoBERTa which is roughly 8 times faster than Longformer but has smaller input size (512 tokens). \n",
    "    - Longformer uses sliding window attention which reduces attention mechanism time complexity to linear O(nw) where w is the window size. \n",
    "- Some strange clusters e.g. ['her friend Clara, who asked her what she had borrowed', 'Clara'] (the first element is overly long)\n",
    "- How to handle overlapping spans in the processing step\n",
    "- There is no given 'representative value' for a given cluster (maybe an LLM processes?)\n",
    "- Some failures (cannot associate \"we\" with \"AISG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan Lee\\.conda\\envs\\aiip\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "11/22/2024 17:02:02 - INFO - \t missing_keys: []\n",
      "11/22/2024 17:02:02 - INFO - \t unexpected_keys: []\n",
      "11/22/2024 17:02:02 - INFO - \t mismatched_keys: []\n",
      "11/22/2024 17:02:02 - INFO - \t error_msgs: []\n",
      "11/22/2024 17:02:02 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "11/22/2024 17:02:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.10 examples/s]\n",
      "11/22/2024 17:02:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Cluster(mentions=[Mention(char_idx=(0, 1), content='We'), Mention(char_idx=(13, 14), content='We')]),\n",
       " Cluster(mentions=[Mention(char_idx=(46, 62), content='the coref package'), Mention(char_idx=(65, 76), content='This package')])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.components.coreference_models import FastCoreferenceModel\n",
    "\n",
    "my_model = FastCoreferenceModel(device=\"cpu\")\n",
    "my_model.predict(text=texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
