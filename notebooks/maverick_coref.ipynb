{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maverick Coref Exploration\n",
    "- [Paper](https://aclanthology.org/2024.acl-long.722/)\n",
    "- [Repo](https://github.com/SapienzaNLP/maverick-coref) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Ryan Lee\\\\Desktop\\\\AISG Internship\\\\rag'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current file's directory (e.g., the 'notebooks' directory)\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# Navigate one level up to project directory\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# Add the directory to sys.path\n",
    "sys.path.append(project_dir)\n",
    "os.chdir(project_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some installation errors for Windows users when you attempt to `pip install maverick-coref`. We will need to update the setup file. \n",
    "\n",
    "- `git clone https://github.com/SapienzaNLP/maverick-coref.git`\n",
    "- `cd maverick-coref`\n",
    "- In `setup.py` explicitly specify to use `utf-8` encoding (otherwise would use system-default encodign): `long_description=open(\"README.md\", encoding=\"utf-8\").read()`\n",
    "- `pip install -e .`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan Lee\\.conda\\envs\\aiip\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sapienzanlp/maverick-mes-ontonotes loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan Lee\\.conda\\envs\\aiip\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ryan Lee\\.conda\\envs\\aiip\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pip install maverick-coref\n",
    "from maverick import Maverick\n",
    "\n",
    "model = Maverick(\n",
    "    hf_name_or_path = \"sapienzanlp/maverick-mes-ontonotes\",\n",
    "    device = \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Barack',\n",
       "  'Obama',\n",
       "  'is',\n",
       "  'traveling',\n",
       "  'to',\n",
       "  'Rome',\n",
       "  '.',\n",
       "  'The',\n",
       "  'city',\n",
       "  'is',\n",
       "  'sunny',\n",
       "  'and',\n",
       "  'the',\n",
       "  'president',\n",
       "  'plans',\n",
       "  'to',\n",
       "  'visit',\n",
       "  'its',\n",
       "  'most',\n",
       "  'important',\n",
       "  'attractions',\n",
       "  '.'],\n",
       " 'clusters_token_offsets': [((5, 5), (7, 8), (17, 17)), ((0, 1), (12, 13))],\n",
       " 'clusters_char_offsets': [[(29, 32), (35, 42), (86, 88)],\n",
       "  [(0, 11), (57, 69)]],\n",
       " 'clusters_token_text': [['Rome', 'The city', 'its'],\n",
       "  ['Barack Obama', 'the president']],\n",
       " 'clusters_char_text': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Barack Obama is traveling to Rome. The city is sunny and the president plans to visit its most important attractions.\"\n",
    "results = model.predict(text)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are AISG. We are so happy to see you using the coref package. This package is very fast!\n",
      "[['We', 'We'], ['the coref package', 'This package']]\n",
      "[((0, 0), (4, 4)), ((12, 14), (16, 17))]\n",
      "===========================================================================================\n",
      "Alice goes down the rabbit hole. Where she would discover a new reality beyond her expectations.\n",
      "[['Alice', 'she', 'her']]\n",
      "[((0, 0), (8, 8), (15, 15))]\n",
      "================================================================================================\n",
      "Mary saw Susan at the park. She was playing with a frisbee. They then conversed.\n",
      "[['Susan', 'She']]\n",
      "[((2, 2), (7, 7))]\n",
      "================================================================================\n",
      "Alice went to the library because she wanted to borrow a book. She found a novel by Kenrick and decided to check it out. As Alice walked home, she bumped into her friend Clara, who asked her what she had borrowed. Alice showed it to Clara, and they talked about the author for a while.\n",
      "[['Alice', 'she', 'She', 'Alice', 'she', 'her', 'her', 'she', 'Alice'], ['a novel by Kenrick', 'it', 'it'], ['her friend Clara , who asked her what she had borrowed', 'Clara'], ['Kenrick', 'the author']]\n",
      "[((0, 0), (6, 6), (13, 13), (27, 27), (31, 31), (34, 34), (40, 40), (42, 42), (46, 46)), ((15, 18), (23, 23), (48, 48)), ((34, 44), (50, 50)), ((18, 18), (56, 57))]\n",
      "=============================================================================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    'We are AISG. We are so happy to see you using the coref package. This package is very fast!',\n",
    "    'Alice goes down the rabbit hole. Where she would discover a new reality beyond her expectations.',\n",
    "    'Mary saw Susan at the park. She was playing with a frisbee. They then conversed.',\n",
    "    'Alice went to the library because she wanted to borrow a book. She found a novel by Kenrick and decided to check it out. As Alice walked home, she bumped into her friend Clara, who asked her what she had borrowed. Alice showed it to Clara, and they talked about the author for a while.'\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    results = model.predict(text)\n",
    "    print(text)\n",
    "    print(results['clusters_token_text'])\n",
    "    print(results['clusters_token_offsets'])\n",
    "    print(\"=\"*len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "- Similar results to fastcoref\n",
    "- All Maverick models use DeBERTa-v3 (both base and large) which can model very long input texts. DeBERTa_large can handle sequences up to 24,528 tokens, making it better for long context. But this is very computationally expensive as attention mechanism incurs quadratic computational complexity (unlike Longformer, which uses sliding window attention mechanism for linear time complexity of O(nw) where w is the window size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sapienzanlp/maverick-mes-ontonotes loading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Cluster(mentions=[Mention(char_idx=(0, 1), content='We'), Mention(char_idx=(13, 14), content='We')]),\n",
       " Cluster(mentions=[Mention(char_idx=(46, 62), content='the coref package'), Mention(char_idx=(65, 76), content='This package')])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.components.coreference_models import MaverickCoreferenceModel\n",
    "\n",
    "my_model = MaverickCoreferenceModel(device=\"cpu\")\n",
    "my_model.predict(texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
